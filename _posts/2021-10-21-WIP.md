<h2>Predicting NFL Draft Round Based on Player's NFL Combine Performance<h2>

 <img src="/assets/img/stats_by_position.png" alt="Look at variance between 'Line' and 'Skill' players">
 
<h3>I. Introduction</h3>
Projecting (and selecting) players for the NFL draft has become a <a href="https://www.cbssports.com/nfl/draft/mock-draft/">year-round focus</a> for certain segments of the sport's fanbase. To gain insight into collegiate player's future performance, NFL executives, bloggers, and TV talking heads spend countless hours
<ul>
 <li>Reviewing tape</li>
 <li>Making assessments of things such as "Motor", "Heart", and "Grit"</li>
 <li>Asking players questions like "If you could be any kind of animal, what would you be?</li>
 </ul>
 
 <br/>
 Which raises the question - when attempting to project a player's placement in the NFL draft, can we derive any insight out of something more quantifiable than an associate GM's hamfisted attempt at pop psychology?
 
 <br/><br/>
 In this essay, I use quantifiable performance data from the NFL Combine to see if we can better predict a player's selection round in the NFL draft.
 
 
<h3>II. Data and Data Wrangling</h3>
To begin this process, I collected NFL Combine performance data from <a href="https://nflcombineresults.com/">NFLCombineResults.com</a>, and collected draft history data from ESPN (example page <a href="http://insider.espn.com/nfl/draft/history/_/year/history?year=2020&round=-1&team=-1&college=-1&x=33&y=23&position=-1&procoach=-1&highschoolstate=-1&award=-1&collegecoach=-1&highschool=-1">here</a>).
 
<br/><br/>
The resulting datasets look like this:
<br/>
<b>NFL Combine Data:</b>
<img src="/assets/img/combine_data_5_rows.png">
 
<br/><br/>
<b>NFL Draft Data:</b>
<img src="/assets/img/draft_data_5_rows.png">


<br/>
These datasets have the raw information we need, but we'll need to merge the datasets together to pair a given player's NFL Combine statistics with their ultimate draft selection. Maybe we can just join by player name? That would require that each name if totally unique. Are they?
<br/><br/>
<code>
 df_combine['name'].value_counts(sort='descending').head(10)<br/>
 Out[7]<br/>
Brandon Williams    5<br/>
Chris Brown         5<br/>
Brian Allen         4<br/>
Mike Williams       4<br/>
Chris Jones         4<br/>
</code>



 
<code>
import json  <br/>
import pandas  <br/>
my_json_looking_text = '{"name":"John", "age":30, "car":null}'  <br/>
my_json = json.loads(my_json_looking_text)  <br/>
df = pd.json_normalize(my_json)  <br/>
</code>
 
 
 <br/>
So there we have it! A unique play id, the home team win percentage at the moment of the play, and the home and away team scores. That's the raw data we'll need to evaluate how well the win probability forecasts hold up when one team has a 99% or higher win probability. We'll throw out the other interesting bits of data like play types, period, down and distance, etc. They may be useful in another analysis, but won't be needed for our question today. And since we're collecting this for 1,343 NFL games, we'll save a lot of data and memory by only focusing on the most critical data columns.

<h4>Data Cleanup and Processing</h4>
We have the raw data we need (gameId, playId, homeWinPercentage, play.homeScore, and play.AwayScore). But We still need to tease out some other calculated metrics, and remove some of the records. Specifically, we need to:
<ol>
 	<li>Identify the last play of each game, which serves two goals:
<ul>
 	<li>The home/away team scores at the last play of the game tell us the winner!</li>
 	<li>We need to remove the last play of the game from our subsequent analysis, since the "Win Percent" for some team is always 100% when the game is over (excluding ties).</li>
</ul>
</li>
 	<li>Filter out remaining play-by-play records where no team has a 99% or higher win percentage.</li>
</ol>

<h5>Identifying the last play of each game</h5>
After scraping play-by-play data for all NFL games from 2016-2020, we have a dataset of 231,575 individual plays. Each <code>playId</code> value is stored as the 9-digit <code>gameId</code> followed by 2-3 additional, always increasing, digits. Therefore, we can identify the last play of every game as the largest (maximum) <code>playId</code> value for any game.  <br/><br/>

<code>
&#35; df_pbp is a dataframe of all play by play data
<br/>&#35; df_pbp.columns -> (playId, gameId, homeWinPercentage, play.homeScore, play.awayScore)
<br/> df_last_play_each_game = df_pbp.groupby('gameId')['playId'].max()
</code>
 
<br/><br/>
Once we've identified the last play of each game, let's capture the final score in another dataframe. Do this by making a copy of the df_pbp dataframe for rows where the <code>playId</code> is found in the "last play" dataframe.
 
 <br/><br/>

<code>
 df_game_results = df_pbp[df_pbp['playId'].
    isin(df_last_play_each_game)][['playId', 'gameId', 'play.homeScore',
                                   'play.awayScore']]
</code>

<br/><br/>
And just to be clear, let's rename the "home" and "away" team scores to indicate that they are final:
 
<br/><br/>
<code>
&#35; Rename scores to indicate "Final"<br/>
df_game_results.rename(columns={'play.homeScore': 'homeFinal',<br/>
                                'play.awayScore': 'awayFinal'},<br/>
                       inplace=True)
</code>

<br/><br/>
 
 
<h5>Keep plays where one team has a 99%+ win probability</h5>
We want to examine all plays where one team has at least a 99% chance of winning. But the last play of every game shows the winning team with a 100% win probability - we need to now discard these entries from our play-by-play dataset. We'll use a new dataframe, df, as our cleaned up, go-forward play by play dataset.
 
<br/><br/>
<code>
 
 df = df_pbp.loc[~df_pbp['playId'].isin(df_game_results['playId'])]
</code>

<br/><br/>
Now we want to focus only on plays/games where one team has at least a 99% win probability at any point before the game is over.
<br/>
We'll make a calculated column "victory proximity" that measures how close ANY team is to winning the game...
 
<br/><br/>
<code>
 df['victory_proximity'] = 
 <br/>np.maximum(df['homeWinPercentage'], 1-df['homeWinPercentage'])
</code>

<br/><br/>
And then we'll throw out any records where this value is under 99%. We'll use a new "df_99" dataframe for this pared-down dataset.

<br/><br/>
<code>
 df_99 = df.loc[df['victory_proximity']>=.99]
</code>

<br/><br/>
Now our <code>df_99</code> dataframe has every (non-final) NFL play where one team has at least a 99% chance of winning, and our <code>df_game_results</code> contains the final score of each game. Let's merge these two together to get the in-game win probability per play alongside the actual, final game results in a single dataframe.  
 
<br/><br/>
<code>
  df_pbp_full = df_99.merge(df_game_results[['gameId', 'homeFinal', 'awayFinal']],<br/>
                                           how='left', on='gameId',<br/>
                      suffixes=[None, None])
</code>
  
<br/><br/>
 
<h3>III. Statistical Methods</h3>
We now want to statistically validate the accuracy of ESPN's win probability. 
 
<br/><br/>
If the odds forecasts are perfectly accurate, we should see that teams with a 99% win probability win 99% of the time (and lose 1%). In our dataset, we have a a column <code>homeWinPercentage</code> and <code>HomeVictory</code> that should therefore have identical means under the null hypothesis. Our dataset included every NFL game with an ESPN win probability, so the actual game outcomes represent a true population mean. We therefore will run a one-sided t-test with the following null hypothesis:<br/><br/>
 
 Ho: x^bar = Mu
 
 <br/><br/>
 To run our test on our data, we execute the following code: <br/><br/>
 
 <code>
  import scipy.stats as st<br/>
  actual_home_win_pct = df_pbp_full['HomeVictory'].mean()<br/>
  t_stat, p_val = st.ttest_1samp(df_pbp_full['homeWinPercentage'], actual_home_win_pct)<br/>
  print(f't_stat {t_stat:.2f}, p_val: {p_val:.2f}')<br/>
  >>> t_stat 0.03, p_val: 0.98<br/><br/>
 </code>
 
<h3>IV. Results</h3>
For my analysis, the P-value returned was 0.98, meaning we are nowhere close to rejecting the null hypothesis. In other words, ESPN's win probability ratings appear to be remarkably well calibrated at projecting 99% and higher win probabilities.
  
<br/><br/>
Essentially, our t-test findings show that "Comeback games" (where a team with less that 1% chance of winning at some point during the game eventually comes back to win) occur about as often as we'd expect if the ESPN odds were correctly calibrated.<br/><br/>
 
So let's look a little closer at just these anomalous comeback games. Below is a chart of how many comeback victories we observe for a given "victory proximity" score. 
 <br/>
<img src="/assets/img/plot_img.png" alt="Historgram of comeback victories by losing team's win probability">
 <br/><br/>
 We would expect this to be a right-tailed distribution, with fewer comeback games associated with a 99.% win probability vs. just a 99% win probability. But the large spike around 99.9% gives me pause - it looks like there are some noisy/erronous data entries around this level. 
 <br/><br/>
 
 Viewing a heatmap with a split by home or away team victory shows a strong, implausible concentration of away-team victories in games/plays where the home team achieve close to a 99.% win probability, yet there are no similar home team games. 
<br/>
<p>
 <img src="/assets/img/heatmap.png" alt="Heatmap of comeback victories by victory proximity">
</p>
 
 <br/>
 This anomalous pattern warrants further investigation into these datapoints. If these clusters of comeback victories near 99.9% turn out to be erroneous, the data should be cleaned and the t-test analysis should be re-run on the cleaner dataset.

 <h3>V. Conclusion</h3>
 According to our t-test on a first pass on ESPN's data, ESPN's win probability scores seem remakably accurate when predicting a 99% win probability. Yet visualizations of the distribtion of "comeback" victories show a higher concentration surprise victories at 99.9% and 100% level, which raises questions about the existence of errors or miscodings in the data. Given that our investigation already focused on a small subset of all plays, just a few miscodings of win probability to extremes of "0" or "1" would drastically alter our results.
 
 <br/><br/>
 So despite an inital finding that ESPN's win probability scores are accurate, I personally plan to do a little more investigation into these data issues. If you're watching a game at a bar this weekend, I'd still advise holding onto your wallet before you take a bet at 99:1 odds just because ESPN.com claims a game is wrapped up.
 
 <div id='pipe1'>
  <style>#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 {color: black;background-color: white;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 pre{padding: 0;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-toggleable {background-color: white;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-item {z-index: 1;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-parallel::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-parallel-item:only-child::after {width: 0;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8 div.sk-container {display: inline-block;position: relative;}</style><div id="sk-4ddf79a3-45fc-4342-8a9e-e2728bdc4cd8" class"sk-top-container"><div class="sk-container"><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="eb357ee1-d441-437e-8865-2cbc5f2be1c1" type="checkbox" ><label class="sk-toggleable__label" for="eb357ee1-d441-437e-8865-2cbc5f2be1c1">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[('ordinalencoder',
                 OrdinalEncoder(cols=['pos_group', 'offense_defense',
                                      'line_or_skill'],
                                mapping=[{'col': 'pos_group',
                                          'data_type': dtype('O'),
                                          'mapping': LB     1
CB     2
S      3
TE     4
RB     5
WR     6
NaN   -2
dtype: int64},
                                         {'col': 'offense_defense',
                                          'data_type': dtype('O'),
                                          'mapping': D      1
O      2
NaN   -2
dtype: int64},
                                         {'col': 'line_or_skill',
                                          'data_type': dtype('O'),
                                          'mapping': S      1
NaN   -2
dtype: int64}])),
                ('decisiontreeclassifier',
                 DecisionTreeClassifier(random_state=42))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="3cbbc6d4-d26f-44e0-b447-511656f36f39" type="checkbox" ><label class="sk-toggleable__label" for="3cbbc6d4-d26f-44e0-b447-511656f36f39">OrdinalEncoder</label><div class="sk-toggleable__content"><pre>OrdinalEncoder(cols=['pos_group', 'offense_defense', 'line_or_skill'],
               mapping=[{'col': 'pos_group', 'data_type': dtype('O'),
                         'mapping': LB     1
CB     2
S      3
TE     4
RB     5
WR     6
NaN   -2
dtype: int64},
                        {'col': 'offense_defense', 'data_type': dtype('O'),
                         'mapping': D      1
O      2
NaN   -2
dtype: int64},
                        {'col': 'line_or_skill', 'data_type': dtype('O'),
                         'mapping': S      1
NaN   -2
dtype: int64}])</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="4e1abd67-122b-4ef1-935b-9db87392be74" type="checkbox" ><label class="sk-toggleable__label" for="4e1abd67-122b-4ef1-935b-9db87392be74">DecisionTreeClassifier</label><div class="sk-toggleable__content"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div></div></div></div></div>
 </div>
